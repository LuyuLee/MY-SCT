{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\Anaconda\\\\Anaconda3\\\\envs\\\\tensorflow\\\\python36.zip', 'D:\\\\Anaconda\\\\Anaconda3\\\\envs\\\\tensorflow\\\\DLLs', 'D:\\\\Anaconda\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib', 'D:\\\\Anaconda\\\\Anaconda3\\\\envs\\\\tensorflow', '', 'D:\\\\Anaconda\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\site-packages', 'D:\\\\Anaconda\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Administrator\\\\.ipython']\n",
      "['D:\\\\Anaconda\\\\Anaconda3\\\\envs\\\\tensorflow\\\\python36.zip', 'D:\\\\Anaconda\\\\Anaconda3\\\\envs\\\\tensorflow\\\\DLLs', 'D:\\\\Anaconda\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib', 'D:\\\\Anaconda\\\\Anaconda3\\\\envs\\\\tensorflow', '', 'D:\\\\Anaconda\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\site-packages', 'D:\\\\Anaconda\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Administrator\\\\.ipython', '/D/Anaconda/CART']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.append('/D/Anaconda/CART')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from numpy import *  # genfromtxt\n",
    "import logisticscrap as lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading from the file using numpy genfromtxt\n",
    "def load_csv(file):\n",
    "    X = genfromtxt(file, delimiter=\",\", dtype=str)\n",
    "    print(X)\n",
    "    return (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data and generate the training labels,training features, test labels and test training\n",
    "def generate_set(X, Pi, CrossTest = 1):\n",
    "    # print(X.shape[0])\n",
    "    Feature_name = X[0, 4:]\n",
    "    # print('Feature_name :', Feature_name)\n",
    "    Label = X[1:, :4]\n",
    "    # print(Label.shape, 'Label :', Label)\n",
    "    Y = Label[:, 2]\n",
    "    # print(Y.shape, \"Y is\", Y)\n",
    "    # print(Y)\n",
    "    j = Y.reshape(len(Y), 1)\n",
    "    # print(j.shape, \"J is\", j)\n",
    "    new_X = X[1:, 4:]\n",
    "    # normalizing the data step\n",
    "    # normalized_X = normalize(new_X)\n",
    "    # print(\"Normal X\",normalized_X)\n",
    "    # https://blog.csdn.net/qq_38150441/article/details/80488800\n",
    "    final_X = np.concatenate((new_X, Label, Pi), axis=1)\n",
    "    print(\"np \", final_X[:4])\n",
    "    X = final_X\n",
    "    size_of_rows = X.shape[0]\n",
    "    # test data size is 10%\n",
    "    num_test = round(0.1 * (X.shape[0]))\n",
    "    start = 0\n",
    "    end = num_test\n",
    "    test_attri_list = []\n",
    "    test_class_names_list = []\n",
    "    training_attri_list = []\n",
    "    training_class_names_list = []\n",
    "    # ten fold cross-validation\n",
    "    if CrossTest == 0:\n",
    "        X_training = X[:, :-6]\n",
    "        X_training = X_training.astype(np.float)\n",
    "        y_training = X[:, -6:]\n",
    "        y_train = y_training.astype(np.float)\n",
    "        training_attri_list.append(X_training)\n",
    "        training_class_names_list.append(y_train)\n",
    "        return Feature_name, None, None, training_attri_list, training_class_names_list\n",
    "    for i in range(10):\n",
    "        X_test = X[start:end, :]\n",
    "        tmp1 = X[:start, :]\n",
    "        tmp2 = X[end:, :]\n",
    "        X_training = np.concatenate((tmp1, tmp2), axis=0)\n",
    "        # X_training = X[:start,:]+ X[end: , :]\n",
    "        y_test = X_test[:, -4:]\n",
    "        # flatten https://blog.csdn.net/liuweiyuxiang/article/details/78220080\n",
    "        # y_test = y_test.flatten()\n",
    "        # print(\"y_test\", y_test)\n",
    "        y_training = X_training[:, -6:]\n",
    "        # y_training = y_training.flatten()\n",
    "        y_train = y_training.astype(np.float)\n",
    "        y_test = y_test.astype(np.float)\n",
    "        X_test = X_test[:, :-6]\n",
    "        X_training = X_training[:, :-6]\n",
    "        X_test = X_test.astype(np.float)\n",
    "        X_training = X_training.astype(np.float)\n",
    "        test_attri_list.append(X_test)\n",
    "        test_class_names_list.append(y_test)\n",
    "        training_attri_list.append(X_training)\n",
    "        training_class_names_list.append(y_train)\n",
    "        # print(\"start is\",start)\n",
    "        # print(\"end is\",end)\n",
    "        start = end\n",
    "        end = end + num_test\n",
    "    # print(\"training_class_names_list\", training_class_names_list)\n",
    "    return Feature_name, test_attri_list, test_class_names_list, training_attri_list, training_class_names_list  # (X_test,y_test,X_training,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992, 11535)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992, 795)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========sigmoid函数转化的值，即：概率p=========\n",
      "[[9.99867578e-01 1.32421973e-04]\n",
      " [2.74321627e-03 9.97256784e-01]\n",
      " [9.99415952e-01 5.84047665e-04]\n",
      " ...\n",
      " [7.43557767e-04 9.99256442e-01]\n",
      " [9.99918312e-01 8.16877227e-05]\n",
      " [9.99880022e-01 1.19977839e-04]]\n",
      "[[9.99867578e-01 1.32421973e-04]\n",
      " [2.74321627e-03 9.97256784e-01]\n",
      " [9.99415952e-01 5.84047665e-04]\n",
      " ...\n",
      " [7.43557767e-04 9.99256442e-01]\n",
      " [9.99918312e-01 8.16877227e-05]\n",
      " [9.99880022e-01 1.19977839e-04]]\n"
     ]
    }
   ],
   "source": [
    "file = 'mydata.csv'\n",
    "Pi = lr.CaculatePi(file)\n",
    "Pi_value = array(Pi)\n",
    "print(Pi_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['vital_status' 'radiation_therapy' 'new_death' ... 'ZWINT' 'ZYG11A'\n",
      "  'psiTPTE22']\n",
      " ['0 ' '0' '4047' ... '0.534733273' '1.208846151' '-2.373823539']\n",
      " ['0 ' '1' '4005' ... '0.577945206' '0.977733872' '1.654037038']\n",
      " ...\n",
      " ['0 ' '1' '488' ... '0.171972936' '0.813046783' '0.709200788']\n",
      " ['0 ' '0' '3287' ... '-1.044003558' '-0.187380517' '0.719903756']\n",
      " ['0 ' '0' '3256' ... '1.71976962' '0.670018274' '-0.934434727']]\n",
      "np  [['0.434870843' '-0.594028145' '0.384618321' ... '0' '0.9998675780269339'\n",
      "  '0.00013242197306607978']\n",
      " ['0.657748071' '-0.594028145' '-0.304375805' ... '0'\n",
      "  '0.0027432162686197303' '0.9972567837313803']\n",
      " ['1.351552381' '-0.594028145' '1.215247867' ... '0' '0.9994159523348022'\n",
      "  '0.0005840476651977986']\n",
      " ['0.397398291' '-0.594028145' '-0.535058303' ... '0'\n",
      "  '0.9999541567079137' '4.584329208639152e-05']]\n",
      "Y =  [array([[0.00000000e+00, 0.00000000e+00, 4.04700000e+03, 0.00000000e+00,\n",
      "        9.99867578e-01, 1.32421973e-04],\n",
      "       [0.00000000e+00, 1.00000000e+00, 4.00500000e+03, 0.00000000e+00,\n",
      "        2.74321627e-03, 9.97256784e-01],\n",
      "       [0.00000000e+00, 0.00000000e+00, 1.47400000e+03, 0.00000000e+00,\n",
      "        9.99415952e-01, 5.84047665e-04],\n",
      "       ...,\n",
      "       [0.00000000e+00, 1.00000000e+00, 4.88000000e+02, 0.00000000e+00,\n",
      "        7.43557767e-04, 9.99256442e-01],\n",
      "       [0.00000000e+00, 0.00000000e+00, 3.28700000e+03, 0.00000000e+00,\n",
      "        9.99918312e-01, 8.16877227e-05],\n",
      "       [0.00000000e+00, 0.00000000e+00, 3.25600000e+03, 0.00000000e+00,\n",
      "        9.99880022e-01, 1.19977839e-04]])]\n"
     ]
    }
   ],
   "source": [
    "newfile = 'mydata.csv'\n",
    "# load the data file and do the preprocessing\n",
    "num_arr = load_csv(newfile)\n",
    "Feature_name, test_attri_list, test_class_names_list, training_attri_list, training_class_names_list = generate_set(num_arr, Pi_value, CrossTest=0)\n",
    "print('Y = ', training_class_names_list )\n",
    "dict_of_input, fea = build_dict_of_attributes_with_class_values(training_attri_list[0], training_class_names_list[0])\n",
    "temptdata = dict_of_input[0]\n",
    "# print(\"input data to get tao = \", temptdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tao_Value = taoob(temptdata)\n",
    "# print('Tao_initial = ', Tao_Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate Tao_value , the input value should be transport by the get_remainder_dict function\n",
    "def taoob (attri_list):\n",
    "    # add a function tao !!\n",
    "    # print(np.array(attri_list).shape)\n",
    "    Treat_acc = 0\n",
    "    number_sample = len(attri_list)\n",
    "    if number_sample ==0:\n",
    "        # print('THIS IS SOME WRONG!!!')\n",
    "        return 0\n",
    "    sum0 = 0\n",
    "    sum1 = 0\n",
    "    tot0 = 0\n",
    "    tot1 = 0 \n",
    "    for i in attri_list:\n",
    "        label = i[1]\n",
    "        Y = label[2]\n",
    "        if label[1]:\n",
    "            print(label[1], label[5])\n",
    "            sum1 += Y / label[5]\n",
    "            tot1 += 1/ label[5]\n",
    "        else: \n",
    "            print(label[1], label[4])\n",
    "            sum0 += Y/ label[4]\n",
    "            tot0 += 1/ label[4]\n",
    "    print('sum0 = ',sum0)\n",
    "    print('sum1 = ',sum1)\n",
    "    print('tot0 = ',tot0)\n",
    "    print('tot1 = ',tot1)\n",
    "    taoob = sum1/tot1 - sum0/tot0\n",
    "    print('taoob : ',taoob)\n",
    "        # print(\"I \", i)\n",
    "        # print(\"I[2] = \", label[2])\n",
    "        # Weight.append(label[1])\n",
    "        # Y.append(label[2])\n",
    "        # if label[1] == 1:\n",
    "        #     Treat_acc += 1\n",
    "    # print(\"Weight: \", Weight)\n",
    "    # print(\"Y: \", Y)\n",
    "    # WeightMat = np.mat(Weight)\n",
    "    # YMat = np.mat(Y)\n",
    "    # print(\"Y: \", YMat)\n",
    "    # pi = Treat_acc / number_sample\n",
    "    '''\n",
    "    print(\"sum Y : \", sum(YMat))\n",
    "    print(\"WeightMat * YMat.T / pi: \", WeightMat * YMat.T / pi)\n",
    "    print(\"sum(WeightMat/pi): \", sum(WeightMat/pi))\n",
    "    print(\"(1-WeightMat) * YMat.T / (1-pi): \", (1-WeightMat) * YMat.T / (1-pi))\n",
    "    print(\"sum((1-WeightMat) / (1-pi)): \", sum((1-WeightMat) / (1-pi)))\n",
    "    '''\n",
    "    # pi_copy = np.shape(np.nonzero(WeightMat == 1)[0])[0]/np.shape(WeightMat)[0]\n",
    "    # print('pi_map = ', pi_map, ' pi_copy = ', pi_copy)\n",
    "    '''\n",
    "    print(\"sum(Weight/pi = \", sum(Weight/pi))\n",
    "    t1 = Weight.T * Y / pi\n",
    "    print('Weight.T * Y / pi= ', t1)\n",
    "    '''\n",
    "    # tao = WeightMat * YMat.T / pi / sum(WeightMat/pi) - (1-WeightMat) * YMat.T / (1-pi) / sum((1-WeightMat) / (1-pi))\n",
    "    #print(\"W=1>{} W=0>{} \". format(Weight.T * Y / pi / sum(Weight/pi), (1-Weight).T * Y / (1-pi) / sum((1-Weight) / (1-pi))))\n",
    "    tao = sum(taoob)\n",
    "    print(\"tao : \", tao)\n",
    "    return tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prework(attri_list):\n",
    "    temptdata = attri_list[0]\n",
    "    # print(\"input data to get tao = \", temptdata)\n",
    "    Tao_Value = taoob(temptdata)\n",
    "    # print('Tao_initial = ', Tao_Value)\n",
    "    return Tao_Value\n",
    "\n",
    "\n",
    "# build a dictionary where the key is the class label and values are the features which belong to that class.\n",
    "def build_dict_of_attributes_with_class_values(X, y):  # ,feature):\n",
    "    dict_of_attri_class_values = {}\n",
    "    fea_list = []\n",
    "    for i in range(X.shape[1]):  #  map all features\n",
    "        fea = i\n",
    "        l = X[:, i]\n",
    "        # print(l)\n",
    "        attribute_list = []\n",
    "        count = 0\n",
    "        # add all features to the dic and add the labal belong to every sample\n",
    "        for j in l:\n",
    "            attribute_value = []\n",
    "            attribute_value.append(j)\n",
    "            attribute_value.append(y[count, :])\n",
    "            attribute_list.append(attribute_value)\n",
    "            count += 1\n",
    "        dict_of_attri_class_values[fea] = attribute_list\n",
    "        fea_list.append(fea)\n",
    "    # print(\"dict_of_attri_class_values: \", dict_of_attri_class_values)\n",
    "    return dict_of_attri_class_values, fea_list\n",
    "    # features_with_max_gain_and_theta(dict_of_attri_class_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class node and explanation is self explaination\n",
    "class Node(object):\n",
    "    def __init__(self, val, lchild, rchild, feature, the, leaf):\n",
    "        self.root_value = val\n",
    "        self.root_left = lchild\n",
    "        self.root_right = rchild\n",
    "        self.feature = feature\n",
    "        self.theta = the\n",
    "        self.leaf = leaf ### bool type\n",
    "\n",
    "    # method to identify if the node is leaf\n",
    "    def is_leaf(self):\n",
    "        return self.leaf\n",
    "\n",
    "    # method to return threshold value\n",
    "    def ret_thetha(self):\n",
    "        return self.theta\n",
    "\n",
    "    def ret_root_value(self):\n",
    "        return self.root_value\n",
    "\n",
    "    def ret_llist(self):\n",
    "        return self.root_left\n",
    "\n",
    "    def ret_rlist(self):\n",
    "        return self.root_right\n",
    "\n",
    "\n",
    "    def ret_feature(self):\n",
    "        return self.feature\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"(%r, %r, %r, %s, %r, %r)\" % (self.root_value, self.root_left, self.root_right, self.feature, self.theta, self.leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree object\n",
    "class DecisionTree(object):\n",
    "    fea_list = []\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root_node = None\n",
    "\n",
    "    # fit the decision tree\n",
    "    def fit(self, dict_of_everything, cl_val, eta_min_val, Feature_name, Tao):\n",
    "        root_node = self.create_decision_tree(dict_of_everything, cl_val, eta_min_val, Feature_name, Tao)  # ,fea_list)\n",
    "        return root_node\n",
    "\n",
    "    # calculate the mean values for all the class labels\n",
    "    def cal_mean_class_values(self, class_values):\n",
    "        mean_val = sum(class_values) / float(len(class_values))\n",
    "        # print(mean_val)\n",
    "        return mean_val\n",
    "\n",
    "    # method to calculate best threshold value for each feature\n",
    "    def cal_best_theta_value(self, ke, attri_list):  ### The atri_listt is the value belong to one feature [1] is the label\n",
    "        data = []\n",
    "        # class_values = []\n",
    "        # print(\"ke is: \", ke)\n",
    "        # print(attri_list, ': is attri_list')\n",
    "        for i in attri_list:\n",
    "            # val = float(i[0])\n",
    "            data.append(i[0])\n",
    "            # class_values.append(i[1])\n",
    "        # print('Classlabel: ', class_values)\n",
    "        # print('data: ', data)\n",
    "        ## We should calculate the tao value here not MSE\n",
    "        # mse_parent = mean_sqaured_error(class_values)\n",
    "        # print(\"mse for parent\",mse_parent)\n",
    "        # print(\"Entropy of parrent\",entropy_of_par_attr)\n",
    "        max_tao = 0\n",
    "        tao_child = []\n",
    "        theta = 0\n",
    "        best_index_left_list = []\n",
    "        best_index_right_list = []\n",
    "        class_labels_list_after_split = []\n",
    "        # print(data)\n",
    "        # data = list(data)\n",
    "        data.sort()\n",
    "        # print('Data: ', data)\n",
    "        for i in range(len(data) - 1):\n",
    "            cur_theta = float(float(data[i]) + float(data[i + 1])) / 2\n",
    "            # print(\"cur thetha\",cur_theta)\n",
    "            # print(data[i] +\"ji\"+ data[i+1],cur_theta)\n",
    "            index_less_than_theta_list = []\n",
    "            values_less_than_theta_list = []\n",
    "            index_greater_than_theta_list = []\n",
    "            values_greater_than_theta_list = []\n",
    "            count = 0\n",
    "            ### enumerate : both index and value:https://blog.csdn.net/liu_xzhen/article/details/79564455\n",
    "            for c, j in enumerate(attri_list):  # print(c,'J is : ', j)\n",
    "                if j[0] <= cur_theta:\n",
    "                    # print(\"J[0] less\", j[0])\n",
    "                    values_less_than_theta_list.append(j)\n",
    "                    index_less_than_theta_list.append(c)\n",
    "                else:\n",
    "                    # print(\"J[0] grater\",j[0])\n",
    "                    values_greater_than_theta_list.append(j)\n",
    "                    index_greater_than_theta_list.append(c)\n",
    "                # count += 1\n",
    "            # print('values_greater_than_theta_list: ', values_greater_than_theta_list)\n",
    "            # print(\"Len og less list\",len(index_less_than_theta_list))\n",
    "            # print(\"len og greater list\",len(index_greater_than_theta_list))\n",
    "            tao_left = taoob(values_less_than_theta_list)\n",
    "            # print(entropy_of_less_attribute)\n",
    "            tao_right = taoob(values_greater_than_theta_list)\n",
    "            ## we use sum sqaure here\n",
    "            tao_split = tao_left ** 2 + tao_right ** 2\n",
    "            if tao_split > max_tao:\n",
    "                max_tao = tao_split\n",
    "                tao_child = [tao_left, tao_right]\n",
    "                theta = cur_theta\n",
    "                best_index_left_list = index_less_than_theta_list\n",
    "                best_index_right_list = index_greater_than_theta_list\n",
    "                class_labels_list_after_split = values_less_than_theta_list + values_greater_than_theta_list\n",
    "        return max_tao, theta, best_index_left_list, best_index_right_list, class_labels_list_after_split, tao_child\n",
    "\n",
    "    # method to select the best feature out of all the features.\n",
    "    ### the dict_rep is\n",
    "    def best_feature(self, dict_rep):\n",
    "        # dict_theta = {}\n",
    "        # dict_theta = {}\n",
    "        key_value = None\n",
    "        best_tao_split = -1\n",
    "        best_theta = 0\n",
    "        best_index_left_list = []\n",
    "        best_index_right_list = []\n",
    "        # best_mse_left = -1\n",
    "        # best_mse_right = -1\n",
    "        best_class_labels_after_split = []\n",
    "        tmp_list = []\n",
    "        best_tao_child = []\n",
    "        for ke in dict_rep.keys():\n",
    "            # print(\"Key now is\", ke, 'dict_rep is ', dict_rep[ke])\n",
    "            tao_split, theta, index_left_list, index_right_list, class_labels_after_split, tao_child = self.cal_best_theta_value(ke, dict_rep[ke])\n",
    "            # print(\"Best theta is\", ke,info_gain,theta,index_left_list)#,index_right_list)\n",
    "            if tao_split > best_tao_split:\n",
    "                best_tao_split = tao_split\n",
    "                best_tao_child = tao_child\n",
    "                best_theta = theta\n",
    "                key_value = ke\n",
    "                best_index_left_list = index_left_list\n",
    "                best_index_right_list = index_right_list\n",
    "                best_class_labels_after_split = class_labels_after_split\n",
    "        tmp_list.append(key_value)\n",
    "        # tmp_list.append(best_info_gain)\n",
    "        tmp_list.append(best_theta)\n",
    "        tmp_list.append(best_index_left_list)\n",
    "        tmp_list.append(best_index_right_list)\n",
    "        tmp_list.append(best_class_labels_after_split)\n",
    "        tmp_list.append(best_tao_child)\n",
    "        return tmp_list\n",
    "\n",
    "    def get_remainder_dict(self, dict_of_everything, index_split):\n",
    "        # global fea_list\n",
    "        splited_dict = {}\n",
    "        for ke in dict_of_everything.keys():\n",
    "            val_list = []\n",
    "            modified_list = []\n",
    "            l = dict_of_everything[ke]\n",
    "            # print(ke,index_left_split)\n",
    "            # print(l)\n",
    "            for i, v in enumerate(l):\n",
    "                # print(i,v)\n",
    "                if i not in index_split:\n",
    "                    # print(ke,i,v)\n",
    "                    modified_list.append(v)\n",
    "                    val_list.append(v[1])\n",
    "            # print(modified_list)\n",
    "            splited_dict[ke] = modified_list\n",
    "        return splited_dict, val_list\n",
    "\n",
    "    # method to create decision tree\n",
    "    def create_decision_tree(self, dict_of_everything, class_val, eta_min_val, Featurename, Tao):  # ,fea_list):\n",
    "        if len(class_val) < eta_min_val:   ### if the number of the leaf < the minest number we having setting\n",
    "            # majority_val = self.cal_mean_class_values(class_val)\n",
    "            # print(\"Leaf node for less than 8 is\",majority_val, len(class_val))#,class_val)\n",
    "            root_node = Node(Tao, None, None, None, None, True)\n",
    "            return root_node\n",
    "        else:\n",
    "            best_features_list = self.best_feature(dict_of_everything)\n",
    "            # print(best_features_list)\n",
    "            node_name = best_features_list[0]\n",
    "            theta = best_features_list[1]\n",
    "            index_left_split = best_features_list[2]\n",
    "            # print(\"Length of left split\",len(index_left_split))#,index_left_split)\n",
    "            index_right_split = best_features_list[3]\n",
    "            # print(\"Length of right split\",len(index_right_split))#,index_right_split)\n",
    "            ## use tao to replace the class_values\n",
    "            class_values = best_features_list[4]\n",
    "            Taovalue = best_features_list[5]\n",
    "            # print (\"Length of class values\", len(class_values))\n",
    "            left_dict, class_val1 = self.get_remainder_dict(dict_of_everything, index_left_split)\n",
    "            # print(\"index of left split\",len(index_left_split))\n",
    "            # print(\"Left class values is\",len(class_val1))\n",
    "            right_dict, class_val2 = self.get_remainder_dict(dict_of_everything, index_right_split)\n",
    "            # print(\"indx of right split\",len(index_right_split))\n",
    "            # print(\"right class values is\",len(class_val2))\n",
    "            ##Add the tao value of each child note here!!!\n",
    "            leftchild = self.create_decision_tree(left_dict, class_val1, eta_min_val, Featurename, Taovalue[0])\n",
    "            # leftchild = None\n",
    "            rightchild = self.create_decision_tree(right_dict, class_val2, eta_min_val, Featurename, Taovalue[1])\n",
    "            root_node = Node(Tao, leftchild, rightchild, Featurename[node_name], theta, False)\n",
    "            return root_node\n",
    "\n",
    "    # method to predict the values for test data\n",
    "    def predict(self, X, root):\n",
    "        predicted_list = []\n",
    "        for row in X:\n",
    "            y_pred = self.classify(row, root)\n",
    "            predicted_list.append(y_pred)\n",
    "        return predicted_list\n",
    "\n",
    "    def classify(self, row, root):\n",
    "        dict_test = {}\n",
    "        for k, j in enumerate(row):\n",
    "            dict_test[k] = j\n",
    "        # print(dict_test)\n",
    "        current_node = root\n",
    "        while not current_node.leaf:\n",
    "            if dict_test[current_node.root_value] <= current_node.theta:\n",
    "                current_node = current_node.root_left\n",
    "            else:\n",
    "                current_node = current_node.root_right\n",
    "        # print(current_node.root_value,dict_test[current_node.root_value], current_node.theta)\n",
    "        return current_node.root_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(Tree, alpha, k, eta_min ):\n",
    "    LeftTree = Tree.root_left\n",
    "    RightTree = Tree.root_right\n",
    "    # pass\n",
    "    if LeftTree.leaf == 0:\n",
    "        LeftTree, k = prune(LeftTree, alpha, k, eta_min)\n",
    "    if RightTree.leaf == 0:\n",
    "        RightTree, k == prune(RightTree, alpha, k, eta_min)\n",
    "    # if there still have child tree, which meaning that parent don't need to prune\n",
    "    if not LeftTree.leaf or not RightTree.leaf:\n",
    "        Tree.root_left = LeftTree\n",
    "        Tree.root_right = RightTree\n",
    "        return Tree, k\n",
    "    else:  # decision if need prune\n",
    "        Q_prune_child = LeftTree.root_value ** 2 + RightTree.root_value ** 2 - alpha * k\n",
    "        Q_prune_parent = Tree.root_value ** 2 - alpha * (k - 1)\n",
    "        if (Q_prune_child - Q_prune_parent) < eta_min:\n",
    "            print('Prune happened! : ')\n",
    "            Tree.leaf = True\n",
    "            Tree.root_left = None\n",
    "            Tree.root_right = None\n",
    "            k -= 1\n",
    "    return Tree, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountChildren (Tree):\n",
    "    Num_node = 0\n",
    "    if Tree.leaf:\n",
    "        # print('Here ',Tree.feature)\n",
    "        Num_node += 1\n",
    "    else:\n",
    "        LeftTree = Tree.root_left\n",
    "        # print('left Tree', LeftTree.root_left)\n",
    "        # print('Tree leaf ', LeftTree.leaf)\n",
    "        RightTree = Tree.root_right\n",
    "        Num_node += CountChildren(LeftTree)\n",
    "        Num_node += CountChildren(RightTree)\n",
    "    # print('Num :', Num_node)\n",
    "    return Num_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main entry for the code\n",
    "def main(num_arr, eta_min):\n",
    "    eta_min_val = round(eta_min * num_arr.shape[0])\n",
    "    print('eta_min_val : ', eta_min_val)\n",
    "    # randomly shuffle the array so that we can divide the data into test/training\n",
    "    # random_arr1 = random_numpy_array(num_arr)\n",
    "    # divide data into test labels,test features,training labels, training features\n",
    "    Feature_name, test_attri_list, test_class_names_list, training_attri_list, training_class_names_list = generate_set(num_arr, CrossTest=0)\n",
    "    accu_count = 0\n",
    "    test_fin_mse = 0\n",
    "    pred_fin = 0\n",
    "    # ten fold iteration for each eta-min value\n",
    "    for i in range(1):\n",
    "        # build a dictionary with class labels and respective features values belonging to that class\n",
    "        dict_of_input, fea = build_dict_of_attributes_with_class_values(training_attri_list[i], training_class_names_list[i])\n",
    "        # instantiate decision tree instance\n",
    "        build_dict = DecisionTree()\n",
    "        Tao_initial = Prework(dict_of_input)\n",
    "        # build the decision tree model.\n",
    "        dec = build_dict.fit(dict_of_input, training_class_names_list[i], eta_min_val, Feature_name, Tao_initial)\n",
    "        # predict the class labels for test features\n",
    "        '''\n",
    "        l = build_dict.predict(test_attri_list[i], dec)\n",
    "        # calculate the mean squared error measure for predicited test data\n",
    "        mse = accuracy_for_predicted_values(test_class_names_list[i], l)\n",
    "        # print(\"Number of right values are\",right,\"Wrong ones are\",wrong)\n",
    "        # accu_count += accu\n",
    "        test_fin_mse += mse\n",
    "        # pred_fin += pred\n",
    "    print(\"Average MSE for eta min of\", eta_min, \"is\", float(test_fin_mse) / 10) \n",
    "    '''\n",
    "    Num_node = CountChildren(dec)\n",
    "    print('Num of node is ', Num_node)\n",
    "    dec = prune(dec, 200, Num_node, eta_min_val)\n",
    "    print(\"Average MSE for eta min of \", eta_min)\n",
    "    print(\"Tree: \", dec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    file = 'mydata.csv'\n",
    "    Pi = lr.CaculatePi(file)\n",
    "    Pi_value = np.mat(Pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CaculatePi() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-4ac728eb5424>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'mydata.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mPi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCaculatePi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mPi_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPi_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: CaculatePi() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newfile = 'test.csv'\n",
    "    # load the data file and do the preprocessing\n",
    "    num_arr = load_csv(newfile)\n",
    "    # for each threshold value run the classifier for 10 cross-validation\n",
    "    eta_min_list = [0.05, 0.10, 0.15, 0.20]\n",
    "    for i in eta_min_list:\n",
    "        main(num_arr, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
